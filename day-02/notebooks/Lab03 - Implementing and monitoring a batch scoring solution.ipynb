{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943b36f3",
   "metadata": {},
   "source": [
    "# Lab03 - Implementing and monitoring a batch scoring solution\n",
    "\n",
    "Azure Machine Learning Batch scoring targets large inference jobs that are not time-sensitive. It is optimized for high-throughput, fire-and-forget inference over large collections of data.\n",
    "\n",
    "In this lab, we will be using [Azure Machine Learning Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines) to  run batch scoring task on a large data set, using a trained model. We will use the `NYC taxi fare predictor` model from Lab 01 to make batch predictions of taxi fares for customer based on a set of input features.  The AML pipeline would entail two steps:\n",
    "\n",
    "1. **Preprocess data step**: In this step we will simulate generation of a large dataset, approximately 250k rows of data. The data generated in this step is passed as an input to the next step.\n",
    "1. **Inference step**: In this step we will use a special type of step called `ParallelRunStep` to do batch scoring on the input data. The `ParallelRunStep` uses a configuration called `ParallelRunConfig` that allows you to control how to break up the scoring job in batches across the available compute nodes and their respective processors. \n",
    "\n",
    "Finally, we will also review how to monitor the batch scoring pipeline runs from within Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fe450",
   "metadata": {},
   "source": [
    "**Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import azureml\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun, StepRun\n",
    "from azureml.pipeline.steps import PythonScriptStep, ParallelRunStep, ParallelRunConfig\n",
    "from azureml.pipeline.core import PipelineParameter, PublishedPipeline\n",
    "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputFileDataset\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed4485",
   "metadata": {},
   "source": [
    "## Connect to the Azure Machine Learning Workspace\n",
    "\n",
    "The AML Python SDK is required for leveraging the experimentation (including pipelines), model management and model deployment capabilities of Azure Machine Learning services. Run the following cell to connect to the AML **Workspace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f2923",
   "metadata": {},
   "source": [
    "### Upload the sample inference data to the blob store\n",
    "\n",
    "The sample dataset has 11,734 rows of input data. We will use this dataset to create a larger input dataset of size around 250k rows in the `preprocess` pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = \"./data\"\n",
    "target_path = \"inference-data\"\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(input_location, \n",
    "                 target_path = target_path, \n",
    "                 overwrite = True, \n",
    "                 show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a561b",
   "metadata": {},
   "source": [
    "### Register the sample inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data_path = DataPath(datastore=datastore, \n",
    "                               path_on_datastore=os.path.join(target_path, \"nyc-taxi-data.csv\"),\n",
    "                               name=\"inference-data\")\n",
    "inference_ds = Dataset.Tabular.from_delimited_files(path=inference_data_path)\n",
    "dataset_name = \"nyc-taxi-inference-dataset\"\n",
    "description = \"Inference dataset to predict NYC taxi fares.\"\n",
    "registered_dataset = inference_ds.register(ws, dataset_name, description=description, create_new_version=True)\n",
    "print('Registered dataset name {} and version {}'.format(registered_dataset.name, registered_dataset.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69923424",
   "metadata": {},
   "source": [
    "### Register the trained NYC taxi fare predictor model\n",
    "\n",
    "The model file is located in the local folder named `model`. We will register this model in the AML model registry. The registered model will be used in the batch inferencing pipeline step to score the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6160f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = \"./model/nyc-taxi-fare.pkl\"\n",
    "model_name = 'nyc-taxi-fare-predictor'\n",
    "model_description = 'Model to predict taxi fares in NYC.'\n",
    "model = Model.register(model_path=model_file_path, \n",
    "                                  model_name=model_name, \n",
    "                                  description=model_description, \n",
    "                                  workspace=ws)\n",
    "model_version = model.version\n",
    "print(\"Model registered: {} \\nModel Description: {} \\nModel Version: {}\".format(model.name, \n",
    "                                                                                model.description, \n",
    "                                                                                model.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca7370",
   "metadata": {},
   "source": [
    "## Create New Compute Cluster\n",
    "\n",
    "AML Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Run the cell below to create a new compute cluster that has 2 nodes, and where each node has 4 cores. We will use this compute cluster to run the pipeline jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2', min_nodes=0, max_nodes=2)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a4081",
   "metadata": {},
   "source": [
    "## Create Pipeline Steps\n",
    "\n",
    "A `Pipeline Step` is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. In this lab we will be using the following built-in Steps:\n",
    "\n",
    "- [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py): A step to run a Python script in a Pipeline. In this lab we will use this type for preprocessing the input data.\n",
    "- [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py): A step to process large amounts of data asynchronously and in parallel. In this lab we will use this type for batch inferencing on a large input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9ded7",
   "metadata": {},
   "source": [
    "### Generate the pipeline step scripts\n",
    "\n",
    "**Run the cell below to create the folder, `scripts`, where we will save the script files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = './scripts'\n",
    "preprocess_filename = 'preprocess.py'\n",
    "inference_filename = 'inference.py'\n",
    "preprocess_file_full_path = os.path.join(source_folder, preprocess_filename)\n",
    "inference_file_full_path = os.path.join(source_folder, inference_filename)\n",
    "os.makedirs(source_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c19c1",
   "metadata": {},
   "source": [
    "**Run the cell below to generate the script for preprocessing the sample inference data**\n",
    "\n",
    "Review the script to see how we take the registered inference dataset and expand it `n` folds to simulate a large dataset that can be then fed to the inference step for batch scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $preprocess_file_full_path\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core import Dataset\n",
    "\n",
    "print(\"In preprocess.py\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"preprocess\")\n",
    "\n",
    "parser.add_argument(\"--dataset_name\", type=str, help=\"dataset name\", dest=\"dataset_name\", required=True)\n",
    "parser.add_argument(\"--n\", type=str, help=\"expansion factor\", dest=\"n\", required=False, default=20)\n",
    "parser.add_argument(\"--output\", type=str, help=\"output directory for processed data\", dest=\"output\", required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1: %s\" % args.dataset_name)\n",
    "print(\"Argument 2: %s\" % args.n)\n",
    "print(\"Argument 3: %s\" % args.output)\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "input_dataset = ws.datasets[args.dataset_name]\n",
    "print('Dataset name {} and version {}'.format(args.dataset_name, input_dataset.version))\n",
    "data = input_dataset.to_pandas_dataframe()\n",
    "print('Inference data loaded!')\n",
    "\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n",
    "            'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', \n",
    "            'temperature', 'normalizeHolidayName', 'isPaidTimeOff']\n",
    "\n",
    "data = data[columns]\n",
    "\n",
    "df = data.copy()\n",
    "print(\"Size of the initial dataframe: {}\".format(len(df)))\n",
    "\n",
    "for i in range(args.n):\n",
    "    data_copy = data.copy()\n",
    "    df = df.append(data_copy, ignore_index=True)\n",
    "\n",
    "print(\"Size of the final dataframe: {}\".format(len(df)))\n",
    "\n",
    "import random\n",
    "import string\n",
    "N = 8\n",
    "print('Adding customer_id')\n",
    "df.loc[:, 'customer_id'] = [''.join(random.choices(string.ascii_uppercase + string.digits, k=N)) \n",
    "                            for i in range(len(df))]\n",
    "\n",
    "from random import randrange\n",
    "print('Adding partition col')\n",
    "df.loc[:, 'partition_id'] = [randrange(10) for i in range(len(df))]\n",
    "\n",
    "print(\"Saving processed file...\")\n",
    "if not (args.output is None):\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "    df.to_parquet(args.output, partition_cols=['partition_id'])\n",
    "    print(\"Saving processed file done!\")\n",
    "else:\n",
    "    print(\"Output directory not found. File not saved!\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05b716",
   "metadata": {},
   "source": [
    "**Run the cell below to generate the script for batch inferencing**\n",
    "\n",
    "Just like a real-time inferencing service, a batch inferencing requires a scoring script to load the model and use it to predict new values. It must include two functions:\n",
    "\n",
    "- init(): Called when the pipeline is initialized.\n",
    "- run(mini_batch): Called for each batch of data to be processed.\n",
    "\n",
    "Typically, you use the init function to load the model from the model registry, and use the run function to generate predictions from each batch of data and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc30840",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $inference_file_full_path\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import azureml.core\n",
    "from azureml.core.model import Model\n",
    "from azureml_user.parallel_run import EntryScript\n",
    "\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n",
    "            'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', \n",
    "            'temperature', 'normalizeHolidayName', 'isPaidTimeOff']\n",
    "\n",
    "def init():\n",
    "    global nyc_taxi_model\n",
    "\n",
    "    logger = EntryScript().logger\n",
    "    logger.info(\"init() is called.\")\n",
    "    print(\"SDK version:\", azureml.core.VERSION)\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\"inference\")\n",
    "    parser.add_argument(\"--model_name\", dest=\"model_name\", required=True, type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model_version\", dest=\"model_version\", required=True, type=int, help=\"model version\")\n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    model_path = Model.get_model_path(model_name=args.model_name, version=args.model_version)\n",
    "    logger.info(\"model path: \" + model_path)\n",
    "    nyc_taxi_model = joblib.load(model_path)\n",
    "    logger.info(\"Model loaded.\")\n",
    "\n",
    "def run(input_data):\n",
    "    logger = EntryScript().logger\n",
    "    logger.info(\"run() is called with number of rows: {}.\".format(len(input_data)))\n",
    "    print(\"Number of rows: {}.\".format(len(input_data)))\n",
    "    \n",
    "    inference_result = nyc_taxi_model.predict(input_data[columns]) # return type is ndarray\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    results['customer_id'] = input_data['customer_id']\n",
    "    results['taxi_fare'] = inference_result\n",
    "\n",
    "    #return list(inference_result) # this needs to be either list or pandas dataframe\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561d7ad",
   "metadata": {},
   "source": [
    "### Create the RunConfiguration for Pipeline Steps\n",
    "\n",
    "A RunConfiguration specify requirements for the pipeline steps such as conda dependencies and docker image. Create and register a custom Inferencing Environment. Use the custom environment in the RunConfiguration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_env = Environment.get(workspace=ws, name='AzureML-Minimal').clone(\"Custom-Batch-Inference-Env\")\n",
    "cd = inference_env.python.conda_dependencies\n",
    "cd.add_pip_package(\"inference-schema\")\n",
    "cd.add_pip_package(\"azureml-dataset-runtime[fuse,pandas]\")\n",
    "cd.add_pip_package(\"numpy\")\n",
    "cd.add_pip_package(\"pandas\")\n",
    "cd.add_pip_package(\"joblib\")\n",
    "cd.add_pip_package(\"scikit-learn==0.24.1\")\n",
    "cd.add_pip_package(\"sklearn-pandas==2.2.0\")\n",
    "inference_env.register(workspace=ws)\n",
    "print('Registered inferencing env.')\n",
    "\n",
    "# Create a new runconfig object\n",
    "run_amlcompute = RunConfiguration()\n",
    "run_amlcompute.environment = inference_env\n",
    "print('Created new run configuration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b16f9",
   "metadata": {},
   "source": [
    "### Create a Preprocess step of type PythonScriptStep\n",
    "\n",
    "A `PythonScriptStep` is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs, and run configuration. In this case, we use a pipeline data object to save the output of the preprocess step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "processed_batch_data = PipelineOutputFileDataset(PipelineData('processed_batch_data', datastore=datastore))\n",
    "print(\"PipelineData object created\")\n",
    "\n",
    "preprocessStep = PythonScriptStep(\n",
    "    name=\"process_data\",\n",
    "    script_name=preprocess_filename, \n",
    "    arguments=[\"--dataset_name\", dataset_name,\n",
    "               \"--output\", processed_batch_data],\n",
    "    outputs=[processed_batch_data],\n",
    "    allow_reuse = False,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_amlcompute,\n",
    "    source_directory=source_folder\n",
    ")\n",
    "print(\"preprocessStep created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f3df8e0",
   "metadata": {},
   "source": [
    "### Create an inference step of type ParallelRunStep\n",
    "\n",
    "The `ParallelRunConfig`, configures how we want to batch the input data and run the scoring script in parallel on available compute cores. The compute cluster provisioned for this lab has a total of 8 cores across two nodes. In this lab, we will use both nodes of the compute cluster, however we will limit to use only 2 of the available 4 cores on each node. The reason for this restriction is for demonstration purpose only, and thus we will be able to see the scoring job spread across both nodes of the compute cluster. The output_action setting for the step is set to \"append_row\", which will ensure that all instances of the step being run in parallel will collate their results to a single output file named ` nyc-taxi-fares.txt`.\n",
    "\n",
    "The `ParallelRunStep` accepts to input parameters: the model name and model version. The step also accepts the output of the preprocess step as input. Also, the output of the step is written to another pipeline data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca472d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "inference_output = PipelineData('inference_output', datastore=datastore)\n",
    "print(\"PipelineData object created\")\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_folder,\n",
    "    entry_script=inference_filename, \n",
    "    mini_batch_size='10MB', #approximate size of mini batch data\n",
    "    error_threshold=10, #acceptable number of record failures\n",
    "    output_action='append_row',\n",
    "    append_row_file_name=\"nyc-taxi-fares.txt\",\n",
    "    environment=inference_env,\n",
    "    compute_target=compute_target, \n",
    "    #Number of nodes to use from the compute target\n",
    "    node_count=2, \n",
    "    #Typically you will not set process_count_per_node and it will default to number of cores on node\n",
    "    #Here we limit to use only 2 of the 4 cores available on each node\n",
    "    process_count_per_node = 2,\n",
    "    run_invocation_timeout=240, #Timeout in seconds for each invocation of the run() method\n",
    "    run_max_try = 3 #The number of maximum tries for a failed or timeout mini batch\n",
    ")\n",
    "\n",
    "inferenceStep = ParallelRunStep(\n",
    "    name=\"inference\", \n",
    "    #Convert PipelineOutputFileDataset to PipelineOutputTabularDataset\n",
    "    inputs=[processed_batch_data.parse_parquet_files()],\n",
    "    output=inference_output,\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--model_name', model_name, '--model_version', model_version],\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "print(\"inferenceStep created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66ad6682",
   "metadata": {},
   "source": [
    "## Create the Batch Scoring Pipeline\n",
    "\n",
    "Since there is a data dependency between the two steps: `preprocessStep` and `inferenceStep`, we only specify the final step in the pipeline. Because of the data dependency the `preprocessStep` will be run prior to running the `inferenceStep`. Run the following cell to create and validate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8da15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline(workspace=ws, steps=[inferenceStep])\n",
    "print (\"Inference Pipeline is built\")\n",
    "\n",
    "inference_pipeline.validate()\n",
    "print(\"Simple validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83a4d9",
   "metadata": {},
   "source": [
    "## Submit the pipeline run\n",
    "\n",
    "The code pattern to submit a run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a40954",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'lab03-exp'\n",
    "pipeline_run = Experiment(ws, experiment_name).submit(inference_pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65e426",
   "metadata": {},
   "source": [
    "### Monitor the Run\n",
    "\n",
    "Using the azureml Jupyter widget, you can monitor the pipeline run. Run the cell below to monitor the experiment run. Wait till the experiment run status is **Completed** before proceeding beyond the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c98457",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f70b1601",
   "metadata": {},
   "source": [
    "## Download and review output\n",
    "\n",
    "Run the cell below to download and review the ouput of the batch scoring pipeline. The results will be downloaded in a local folder named: `azureml`. The output will print every 25k row, and each row will print customer id and the predicted taxi fare. In the end the output will also print the total number of rows (customer fares) that were scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pipeline_run.find_step_run('inference')[0].get_output_data('inference_output')\n",
    "data.download('.', show_progress=True)\n",
    "data_path = os.path.join('./', data.path_on_datastore, 'nyc-taxi-fares.txt')\n",
    "with open(data_path) as fp:\n",
    "    line = fp.readline()\n",
    "    cnt = 1\n",
    "    while line:\n",
    "        if cnt%25000 == 0:\n",
    "            print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "        line = fp.readline()\n",
    "        cnt += 1\n",
    "print(\"Total number of rows processed: {}\".format(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a04ff",
   "metadata": {},
   "source": [
    "## Monitor Batch Scoring Experiment Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271a75a",
   "metadata": {},
   "source": [
    "You can monitor your batch inferencing pipeline runs from within Azure Machine Learning Studio. From the pipeline run details you can navigate to each pipeline step run details and monitor and review the various log files. Below we can look at how to access the **EntryScript** logs that we generated within our inference scriptâ€™s **init** and **run** methods.\n",
    "\n",
    "Run the cell below and then **right-click** on **Link to Azure Machine Learning studio** link below to open the `AML Experiment Run Details` page in a **new browser tab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62972173",
   "metadata": {},
   "source": [
    "- From the **Pileline Run Details** page select **Steps**\n",
    "\n",
    "![Pipeline Run Details](./images/monitor-logs01.png 'Pipeline Run Details')\n",
    "\n",
    "- From the **Steps** tab, select the step: **inference**\n",
    "    \n",
    "![Pipeline Run Details - Pipeline Steps](./images/monitor-logs02.png 'Pipleline Steps')\n",
    "\n",
    "- In the **Outputs + logs** tab, navigate to **logs, user, entry_script_logs, first folder, first log file**\n",
    "    \n",
    "![Inference Step Run Details - Outputs + logs](./images/monitor-logs03.png 'Inference Step Outputs + Logs')\n",
    "\n",
    "Recall that our compute cluster has a maximum of 2 nodes, and each node has 4 cores. However, in **parallel_run_config** we restricted to use only **2 cores per node**. Thus, as a result there are two log files (one for each core) per node (organized by folder).  In the entry script logs you can observe that the scoring script was run twice on this particular core. And the batch size compute resulted in approximately **25k** rows. You will also observe that for some of the cores the entry script was called three times. Overall the entry script was called 10 times across all 4 cores, processing approximately **250k** rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafb938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
